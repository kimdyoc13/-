{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 잠재 요인 협업 필터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강을 이용한 행렬 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 원본 행렬 R 생성, 분해 행렬 P와 Q 초기화, 잠재요인 차원 K는 3 설정. \n",
    "R = np.array([[4, np.NaN, np.NaN, 2, np.NaN ],\n",
    "              [np.NaN, 5, np.NaN, 3, 1 ],\n",
    "              [np.NaN, np.NaN, 3, 4, 4 ],\n",
    "              [5, 2, 1, 2, np.NaN ]])\n",
    "num_users, num_items = R.shape\n",
    "K=3\n",
    "\n",
    "# P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 random한 값으로 입력합니다. \n",
    "np.random.seed(1)\n",
    "P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "Q = np.random.normal(scale=1./K, size=(num_items, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_rmse(R, P, Q, non_zeros):\n",
    "    error = 0\n",
    "    # 두개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성\n",
    "    full_pred_matrix = np.dot(P, Q.T)\n",
    "    \n",
    "    # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
    "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
    "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
    "    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
    "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
    "      \n",
    "    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### iteration step :  0  rmse :  3.2388050277987723\n",
      "### iteration step :  50  rmse :  0.4876723101369648\n",
      "### iteration step :  100  rmse :  0.1564340384819247\n",
      "### iteration step :  150  rmse :  0.07455141311978046\n",
      "### iteration step :  200  rmse :  0.04325226798579314\n",
      "### iteration step :  250  rmse :  0.029248328780878973\n",
      "### iteration step :  300  rmse :  0.022621116143829466\n",
      "### iteration step :  350  rmse :  0.019493636196525135\n",
      "### iteration step :  400  rmse :  0.018022719092132704\n",
      "### iteration step :  450  rmse :  0.01731968595344266\n",
      "### iteration step :  500  rmse :  0.016973657887570753\n",
      "### iteration step :  550  rmse :  0.016796804595895633\n",
      "### iteration step :  600  rmse :  0.01670132290188466\n",
      "### iteration step :  650  rmse :  0.01664473691247669\n",
      "### iteration step :  700  rmse :  0.016605910068210026\n",
      "### iteration step :  750  rmse :  0.016574200475705\n",
      "### iteration step :  800  rmse :  0.01654431582921597\n",
      "### iteration step :  850  rmse :  0.01651375177473524\n",
      "### iteration step :  900  rmse :  0.01648146573819501\n",
      "### iteration step :  950  rmse :  0.016447171683479155\n"
     ]
    }
   ],
   "source": [
    "# R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트에 저장. \n",
    "non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]\n",
    "\n",
    "steps=1000\n",
    "learning_rate=0.01\n",
    "r_lambda=0.01\n",
    "\n",
    "# SGD 기법으로 P와 Q 매트릭스를 계속 업데이트. \n",
    "for step in range(steps):\n",
    "    for i, j, r in non_zeros:\n",
    "        # 실제 값과 예측 값의 차이인 오류 값 구함\n",
    "        eij = r - np.dot(P[i, :], Q[j, :].T)\n",
    "        # Regularization을 반영한 SGD 업데이트 공식 적용\n",
    "        P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])\n",
    "        Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])\n",
    "\n",
    "    rmse = get_rmse(R, P, Q, non_zeros)\n",
    "    if (step % 50) == 0 :\n",
    "        print(\"### iteration step : \", step,\" rmse : \", rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 행렬:\n",
      " [[3.991 0.897 1.306 2.002 1.663]\n",
      " [6.696 4.978 0.979 2.981 1.003]\n",
      " [6.677 0.391 2.987 3.977 3.986]\n",
      " [4.968 2.005 1.006 2.017 1.14 ]]\n"
     ]
    }
   ],
   "source": [
    "pred_matrix = np.dot(P, Q.T)\n",
    "print('예측 행렬:\\n', np.round(pred_matrix, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 컨텐츠 기반 필터링 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tmdb-5000-movie-dataset/tmdb_5000_movies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m; warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m movies \u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tmdb-5000-movie-dataset/tmdb_5000_movies.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(movies\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m movies\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rlaal\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\rlaal\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\rlaal\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\rlaal\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\rlaal\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tmdb-5000-movie-dataset/tmdb_5000_movies.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "movies =pd.read_csv('./tmdb-5000-movie-dataset/tmdb_5000_movies.csv')\n",
    "print(movies.shape)\n",
    "movies.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies[['id','title', 'genres', 'vote_average', 'vote_count',\n",
    "                 'popularity', 'keywords', 'overview']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 100)\n",
    "movies_df[['genres','keywords']][:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "movies_df['genres'] = movies_df['genres'].apply(literal_eval)\n",
    "movies_df['keywords'] = movies_df['keywords'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['genres'] = movies_df['genres'].apply(lambda x : [ y['name'] for y in x])\n",
    "movies_df['keywords'] = movies_df['keywords'].apply(lambda x : [ y['name'] for y in x])\n",
    "movies_df[['genres', 'keywords']][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer를 적용하기 위해 공백문자로 word 단위가 구분되는 문자열로 변환. \n",
    "movies_df['genres_literal'] = movies_df['genres'].apply(lambda x : (' ').join(x))\n",
    "count_vect = CountVectorizer(min_df=0, ngram_range=(1,2))\n",
    "genre_mat = count_vect.fit_transform(movies_df['genres_literal'])\n",
    "print(genre_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "genre_sim = cosine_similarity(genre_mat, genre_mat)\n",
    "print(genre_sim.shape)\n",
    "print(genre_sim[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]\n",
    "print(genre_sim_sorted_ind[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_movie(df, sorted_ind, title_name, top_n=10):\n",
    "    \n",
    "    # 인자로 입력된 movies_df DataFrame에서 'title' 컬럼이 입력된 title_name 값인 DataFrame추출\n",
    "    title_movie = df[df['title'] == title_name]\n",
    "    \n",
    "    # title_named을 가진 DataFrame의 index 객체를 ndarray로 반환하고 \n",
    "    # sorted_ind 인자로 입력된 genre_sim_sorted_ind 객체에서 유사도 순으로 top_n 개의 index 추출\n",
    "    title_index = title_movie.index.values\n",
    "    similar_indexes = sorted_ind[title_index, :(top_n)]\n",
    "    \n",
    "    # 추출된 top_n index들 출력. top_n index는 2차원 데이터 임. \n",
    "    #dataframe에서 index로 사용하기 위해서 1차원 array로 변경\n",
    "    print(similar_indexes)\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "    \n",
    "    return df.iloc[similar_indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',10)\n",
    "similar_movies[['title', 'vote_average']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[['title','vote_average','vote_count']].sort_values('vote_average', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = movies_df['vote_average'].mean()\n",
    "m = movies_df['vote_count'].quantile(0.6)\n",
    "print('C:',round(C,3), 'm:',round(m,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 0.6\n",
    "m = movies_df['vote_count'].quantile(percentile)\n",
    "C = movies_df['vote_average'].mean()\n",
    "\n",
    "def weighted_vote_average(record):\n",
    "    v = record['vote_count']\n",
    "    R = record['vote_average']\n",
    "    \n",
    "    return ( (v/(v+m)) * R ) + ( (m/(m+v)) * C )   \n",
    "\n",
    "movies_df['weighted_vote'] = movies_df.apply(weighted_vote_average, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[['title','vote_average','weighted_vote','vote_count']].sort_values('weighted_vote',\n",
    "                                                                          ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_movie(df, sorted_ind, title_name, top_n=10):\n",
    "    title_movie = df[df['title'] == title_name]\n",
    "    title_index = title_movie.index.values\n",
    "    \n",
    "    # top_n의 2배에 해당하는 쟝르 유사성이 높은 index 추출 \n",
    "    similar_indexes = sorted_ind[title_index, :(top_n*2)]\n",
    "    similar_indexes = similar_indexes.reshape(-1)\n",
    "# 기준 영화 index는 제외\n",
    "    similar_indexes = similar_indexes[similar_indexes != title_index]\n",
    "    \n",
    "    # top_n의 2배에 해당하는 후보군에서 weighted_vote 높은 순으로 top_n 만큼 추출 \n",
    "    return df.iloc[similar_indexes].sort_values('weighted_vote', ascending=False)[:top_n]\n",
    "\n",
    "similar_movies = find_sim_movie(movies_df, genre_sim_sorted_ind, 'The Godfather',10)\n",
    "similar_movies[['title', 'vote_average', 'weighted_vote']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 아이템 기반"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "print(movies.shape)\n",
    "print(ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "ratings_matrix = ratings.pivot_table('rating', index='userId', columns='movieId')\n",
    "ratings_matrix.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title 컬럼을 얻기 이해 movies 와 조인 수행\n",
    "rating_movies = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# columns='title' 로 title 컬럼으로 pivot 수행. \n",
    "ratings_matrix = rating_movies.pivot_table('rating', index='userId', columns='title')\n",
    "\n",
    "# NaN 값을 모두 0 으로 변환\n",
    "ratings_matrix = ratings_matrix.fillna(0)\n",
    "ratings_matrix.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영화와 영화들 간 유사도 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratings_matrix_T = ratings_matrix.transpose()\n",
    "ratings_matrix_T.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "item_sim = cosine_similarity(ratings_matrix_T, ratings_matrix_T)\n",
    "\n",
    "# cosine_similarity() 로 반환된 넘파이 행렬을 영화명을 매핑하여 DataFrame으로 변환\n",
    "item_sim_df = pd.DataFrame(data=item_sim, index=ratings_matrix.columns,\n",
    "                          columns=ratings_matrix.columns)\n",
    "print(item_sim_df.shape)\n",
    "item_sim_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim_df[\"Godfather, The (1972)\"].sort_values(ascending=False)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim_df[\"Inception (2010)\"].sort_values(ascending=False)[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아이템 기반 인접 이웃 협업 필터링으로 개인화된 영화 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(ratings_arr, item_sim_arr ):\n",
    "    ratings_pred = ratings_arr.dot(item_sim_arr)/ np.array([np.abs(item_sim_arr).sum(axis=1)])\n",
    "    return ratings_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pred = predict_rating(ratings_matrix.values , item_sim_df.values)\n",
    "ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index,\n",
    "                                   columns = ratings_matrix.columns)\n",
    "ratings_pred_matrix.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 사용자가 평점을 부여한 영화에 대해서만 예측 성능 평가 MSE 를 구함. \n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)\n",
    "\n",
    "print('아이템 기반 모든 인접 이웃 MSE: ', get_mse(ratings_pred, ratings_matrix.values ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating_topsim(ratings_arr, item_sim_arr, n=20):\n",
    "    # 사용자-아이템 평점 행렬 크기만큼 0으로 채운 예측 행렬 초기화\n",
    "    pred = np.zeros(ratings_arr.shape)\n",
    "\n",
    "    # 사용자-아이템 평점 행렬의 열 크기만큼 Loop 수행. \n",
    "    for col in range(ratings_arr.shape[1]):\n",
    "        # 유사도 행렬에서 유사도가 큰 순으로 n개 데이터 행렬의 index 반환\n",
    "        top_n_items = [np.argsort(item_sim_arr[:, col])[:-n-1:-1]]\n",
    "        # 개인화된 예측 평점을 계산\n",
    "        for row in range(ratings_arr.shape[0]):\n",
    "            pred[row, col] = item_sim_arr[col, :][top_n_items].dot(ratings_arr[row, :][top_n_items].T) \n",
    "            pred[row, col] /= np.sum(np.abs(item_sim_arr[col, :][top_n_items]))        \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pred = predict_rating_topsim(ratings_matrix.values , item_sim_df.values, n=20)\n",
    "print('아이템 기반 인접 TOP-20 이웃 MSE: ', get_mse(ratings_pred, ratings_matrix.values ))\n",
    "\n",
    "\n",
    "# 계산된 예측 평점 데이터는 DataFrame으로 재생성\n",
    "ratings_pred_matrix = pd.DataFrame(data=ratings_pred, index= ratings_matrix.index,\n",
    "                                   columns = ratings_matrix.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rating_id = ratings_matrix.loc[9, :]\n",
    "user_rating_id[ user_rating_id > 0].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unseen_movies(ratings_matrix, userId):\n",
    "    # userId로 입력받은 사용자의 모든 영화정보 추출하여 Series로 반환함. \n",
    "    # 반환된 user_rating 은 영화명(title)을 index로 가지는 Series 객체임. \n",
    "    user_rating = ratings_matrix.loc[userId,:]\n",
    "    \n",
    "    # user_rating이 0보다 크면 기존에 관람한 영화임. 대상 index를 추출하여 list 객체로 만듬\n",
    "    already_seen = user_rating[ user_rating > 0].index.tolist()\n",
    "    \n",
    "    # 모든 영화명을 list 객체로 만듬. \n",
    "    movies_list = ratings_matrix.columns.tolist()\n",
    "    \n",
    "    # list comprehension으로 already_seen에 해당하는 movie는 movies_list에서 제외함. \n",
    "    unseen_list = [ movie for movie in movies_list if movie not in already_seen]\n",
    "    \n",
    "    return unseen_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomm_movie_by_userid(pred_df, userId, unseen_list, top_n=10):\n",
    "    # 예측 평점 DataFrame에서 사용자id index와 unseen_list로 들어온 영화명 컬럼을 추출하여\n",
    "    # 가장 예측 평점이 높은 순으로 정렬함. \n",
    "    recomm_movies = pred_df.loc[userId, unseen_list].sort_values(ascending=False)[:top_n]\n",
    "    return recomm_movies\n",
    "    \n",
    "# 사용자가 관람하지 않는 영화명 추출   \n",
    "unseen_list = get_unseen_movies(ratings_matrix, 9)\n",
    "\n",
    "# 아이템 기반의 인접 이웃 협업 필터링으로 영화 추천 \n",
    "recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)\n",
    "\n",
    "# 평점 데이타를 DataFrame으로 생성. \n",
    "recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])\n",
    "recomm_movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 행렬 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_rmse(R, P, Q, non_zeros):\n",
    "    error = 0\n",
    "    # 두개의 분해된 행렬 P와 Q.T의 내적 곱으로 예측 R 행렬 생성\n",
    "    full_pred_matrix = np.dot(P, Q.T)\n",
    "    \n",
    "    # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출\n",
    "    x_non_zero_ind = [non_zero[0] for non_zero in non_zeros]\n",
    "    y_non_zero_ind = [non_zero[1] for non_zero in non_zeros]\n",
    "    R_non_zeros = R[x_non_zero_ind, y_non_zero_ind]\n",
    "    \n",
    "    full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind]\n",
    "      \n",
    "    mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R, K, steps=200, learning_rate=0.01, r_lambda = 0.01):\n",
    "    num_users, num_items = R.shape\n",
    "    # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 랜덤한 값으로 입력합니다. \n",
    "    np.random.seed(1)\n",
    "    P = np.random.normal(scale=1./K, size=(num_users, K))\n",
    "    Q = np.random.normal(scale=1./K, size=(num_items, K))\n",
    "       \n",
    "    # R > 0 인 행 위치, 열 위치, 값을 non_zeros 리스트 객체에 저장. \n",
    "    non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] > 0 ]\n",
    "   \n",
    "    # SGD기법으로 P와 Q 매트릭스를 계속 업데이트. \n",
    "    for step in range(steps):\n",
    "        for i, j, r in non_zeros:\n",
    "            # 실제 값과 예측 값의 차이인 오류 값 구함\n",
    "            eij = r - np.dot(P[i, :], Q[j, :].T)\n",
    "            # Regularization을 반영한 SGD 업데이트 공식 적용\n",
    "            P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:])\n",
    "            Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:])\n",
    "       \n",
    "        rmse = get_rmse(R, P, Q, non_zeros)\n",
    "        if (step % 10) == 0 :\n",
    "            print(\"### iteration step : \", step,\" rmse : \", rmse)\n",
    "            \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "ratings_matrix = ratings.pivot_table('rating', index='userId', columns='movieId')\n",
    "\n",
    "# title 컬럼을 얻기 이해 movies 와 조인 수행\n",
    "rating_movies = pd.merge(ratings, movies, on='movieId')\n",
    "\n",
    "# columns='title' 로 title 컬럼으로 pivot 수행. \n",
    "ratings_matrix = rating_movies.pivot_table('rating', index='userId', columns='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, Q = matrix_factorization(ratings_matrix.values, K=50, steps=200, learning_rate=0.01, r_lambda = 0.01)\n",
    "pred_matrix = np.dot(P, Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_pred_matrix = pd.DataFrame(data=pred_matrix, index= ratings_matrix.index,\n",
    "                                   columns = ratings_matrix.columns)\n",
    "\n",
    "ratings_pred_matrix.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unseen_movies(ratings_matrix, userId):\n",
    "    # userId로 입력받은 사용자의 모든 영화정보 추출하여 Series로 반환함. \n",
    "    # 반환된 user_rating 은 영화명(title)을 index로 가지는 Series 객체임. \n",
    "    user_rating = ratings_matrix.loc[userId,:]\n",
    "    \n",
    "    # user_rating이 0보다 크면 기존에 관람한 영화임. 대상 index를 추출하여 list 객체로 만듬\n",
    "    already_seen = user_rating[ user_rating > 0].index.tolist()\n",
    "    \n",
    "    # 모든 영화명을 list 객체로 만듬. \n",
    "    movies_list = ratings_matrix.columns.tolist()\n",
    "    \n",
    "    # list comprehension으로 already_seen에 해당하는 movie는 movies_list에서 제외함. \n",
    "    unseen_list = [ movie for movie in movies_list if movie not in already_seen]\n",
    "    \n",
    "    return unseen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomm_movie_by_userid(pred_df, userId, unseen_list, top_n=10):\n",
    "    # 예측 평점 DataFrame에서 사용자id index와 unseen_list로 들어온 영화명 컬럼을 추출하여\n",
    "    # 가장 예측 평점이 높은 순으로 정렬함. \n",
    "    recomm_movies = pred_df.loc[userId, unseen_list].sort_values(ascending=False)[:top_n]\n",
    "    return recomm_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자가 관람하지 않는 영화명 추출   \n",
    "unseen_list = get_unseen_movies(ratings_matrix, 9)\n",
    "\n",
    "# 아이템 기반의 인접 이웃 협업 필터링으로 영화 추천 \n",
    "recomm_movies = recomm_movie_by_userid(ratings_pred_matrix, 9, unseen_list, top_n=10)\n",
    "\n",
    "# 평점 데이타를 DataFrame으로 생성. \n",
    "recomm_movies = pd.DataFrame(data=recomm_movies.values,index=recomm_movies.index,columns=['pred_score'])\n",
    "recomm_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 파이썬 추천 시스템 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suprise 를 이용한 추천 시스템 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement suprise (from versions: none)\n",
      "ERROR: No matching distribution found for suprise\n"
     ]
    }
   ],
   "source": [
    "pip install suprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(surprise\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import surprise \n",
    "\n",
    "print(surprise.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVD\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset \n",
    "from surprise import accuracy \n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.load_builtin('ml-100k')\n",
    "# 수행 시마다 동일하게 데이터를 분할하기 위해 random_state 값 부여\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(random_state=0)\n",
    "algo.fit(trainset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test( testset )\n",
    "print('prediction type :',type(predictions), ' size:',len(predictions))\n",
    "print('prediction 결과의 최초 5개 추출')\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ (pred.uid, pred.iid, pred.est) for pred in predictions[:3] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 아이디, 아이템 아이디는 문자열로 입력해야 함. \n",
    "uid = str(196)\n",
    "iid = str(302)\n",
    "pred = algo.predict(uid, iid)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suprise 주요 모듈 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "# ratings_noh.csv 파일로 unload 시 index 와 header를 모두 제거한 새로운 파일 생성.  \n",
    "ratings.to_csv('./ml-latest-small/ratings_noh.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(0.5, 5))\n",
    "data=Dataset.load_from_file('./ml-latest-small/ratings_noh.csv',reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25, random_state=0)\n",
    "\n",
    "# 수행시마다 동일한 결과 도출을 위해 random_state 설정 \n",
    "algo = SVD(n_factors=50, random_state=0)\n",
    "\n",
    "# 학습 데이터 세트로 학습 후 테스트 데이터 세트로 평점 예측 후 RMSE 평가\n",
    "algo.fit(trainset) \n",
    "predictions = algo.test( testset )\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Reader, Dataset\n",
    "\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv') \n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# ratings DataFrame 에서 컬럼은 사용자 아이디, 아이템 아이디, 평점 순서를 지켜야 합니다. \n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.25, random_state=0)\n",
    "\n",
    "algo = SVD(n_factors=50, random_state=0)\n",
    "algo.fit(trainset) \n",
    "predictions = algo.test( testset )\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교차 검증과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# 판다스 DataFrame에서 Surprise 데이터 세트로 데이터 로딩\n",
    "ratings = pd.read_csv('./ml-latest-small/ratings.csv') # reading data in pandas df\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "algo = SVD(random_state=0)\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# 최적화할 파라미터를 딕셔너리 형태로 지정.\n",
    "param_grid = {'n_epochs': [20, 40, 60], 'n_factors': [50, 100, 200] }\n",
    "\n",
    "# CV를 3개 폴드 세트로 지정, 성능 평가는 rmse, mse로 수행하도록 GridSearchCV 구성\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "# 최고 RMSE Evaluation 점수와 그때의 하이퍼 파라미터\n",
    "print(gs.best_score['rmse'])\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suprise 를 이용한 개인화 영화 추천 시스템 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 코드는 train_test_split( )으로 분리되지 않는 데이터 세트에 fit( )을 호출해 오류가 발생합니다.\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "algo = SVD(n_factors=50, random_state=0)\n",
    "algo.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.dataset import DatasetAutoFolds\n",
    "\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', rating_scale=(0.5, 5))\n",
    "# DatasetAutoFolds 클래스를 ratings_noh.csv 파일 기반으로 생성. \n",
    "data_folds = DatasetAutoFolds(ratings_file='./ml-latest-small/ratings_noh.csv', reader=reader)\n",
    "\n",
    "#전체 데이터를 학습데이터로 생성함. \n",
    "trainset = data_folds.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = SVD(n_epochs=20, n_factors=50, random_state=0)\n",
    "algo.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화에 대한 상세 속성 정보 DataFrame로딩\n",
    "movies = pd.read_csv('./ml-latest-small/movies.csv')\n",
    "\n",
    "# userId=9 의 movieId 데이터 추출하여 movieId=42 데이터가 있는지 확인. \n",
    "movieIds = ratings[ratings['userId']==9]['movieId']\n",
    "if movieIds[movieIds==42].count() == 0:\n",
    "    print('사용자 아이디 9는 영화 아이디 42의 평점 없음')\n",
    "\n",
    "print(movies[movies['movieId']==42])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = str(9)\n",
    "iid = str(42)\n",
    "\n",
    "pred = algo.predict(uid, iid, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unseen_surprise(ratings, movies, userId):\n",
    "    #입력값으로 들어온 userId에 해당하는 사용자가 평점을 매긴 모든 영화를 리스트로 생성\n",
    "    seen_movies = ratings[ratings['userId']== userId]['movieId'].tolist()\n",
    "    \n",
    "    # 모든 영화들의 movieId를 리스트로 생성. \n",
    "    total_movies = movies['movieId'].tolist()\n",
    "    \n",
    "    # 모든 영화들의 movieId중 이미 평점을 매긴 영화의 movieId를 제외하여 리스트로 생성\n",
    "    unseen_movies= [movie for movie in total_movies if movie not in seen_movies]\n",
    "    print('평점 매긴 영화수:',len(seen_movies), '추천대상 영화수:',len(unseen_movies), \\\n",
    "          '전체 영화수:',len(total_movies))\n",
    "    \n",
    "    return unseen_movies\n",
    "\n",
    "unseen_movies = get_unseen_surprise(ratings, movies, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomm_movie_by_surprise(algo, userId, unseen_movies, top_n=10):\n",
    "    # 알고리즘 객체의 predict() 메서드를 평점이 없는 영화에 반복 수행한 후 결과를 list 객체로 저장\n",
    "    predictions = [algo.predict(str(userId), str(movieId)) for movieId in unseen_movies]\n",
    "    \n",
    "    # predictions list 객체는 surprise의 Predictions 객체를 원소로 가지고 있음.\n",
    "    # [Prediction(uid='9', iid='1', est=3.69), Prediction(uid='9', iid='2', est=2.98),,,,]\n",
    "    # 이를 est 값으로 정렬하기 위해서 아래의 sortkey_est 함수를 정의함.\n",
    "    # sortkey_est 함수는 list 객체의 sort() 함수의 키 값으로 사용되어 정렬 수행.\n",
    "    def sortkey_est(pred):\n",
    "        return pred.est\n",
    "    \n",
    "    # sortkey_est( ) 반환값의 내림 차순으로 정렬 수행하고 top_n개의 최상위 값 추출.\n",
    "    predictions.sort(key=sortkey_est, reverse=True)\n",
    "    top_predictions= predictions[:top_n]\n",
    "    \n",
    "    # top_n으로 추출된 영화의 정보 추출. 영화 아이디, 추천 예상 평점, 제목 추출\n",
    "    top_movie_ids = [ int(pred.iid) for pred in top_predictions]\n",
    "    top_movie_rating = [ pred.est for pred in top_predictions]\n",
    "    top_movie_titles = movies[movies.movieId.isin(top_movie_ids)]['title']\n",
    "    top_movie_preds = [ (id, title, rating) for id, title, rating in zip(top_movie_ids, top_movie_titles, top_movie_rating)]\n",
    "    \n",
    "    return top_movie_preds\n",
    "\n",
    "unseen_movies = get_unseen_surprise(ratings, movies, 9)\n",
    "top_movie_preds = recomm_movie_by_surprise(algo, 9, unseen_movies, top_n=10)\n",
    "print('##### Top-10 추천 영화 리스트 #####')\n",
    "\n",
    "for top_movie in top_movie_preds:\n",
    "    print(top_movie[1], \":\", top_movie[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
